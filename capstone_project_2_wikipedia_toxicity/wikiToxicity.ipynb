{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snikhil17/NLP_Simplilearn/blob/main/capstone_project_2_wikipedia_toxicity/wikiToxicity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG0asDoiqgTy"
      },
      "source": [
        "## **Problem Context**\n",
        "#### **Description:** \n",
        "- Using NLP and machine learning, make a model to identify toxic comments from the Talk edit pages on Wikipedia. Help identify the words that make a comment toxic.\n",
        "\n",
        "#### **Statement:**  \n",
        "- Wikipedia is the world’s largest and most popular reference work on the internet with about 500 million unique visitors per month. It also has millions of contributors who can make edits to pages. The Talk edit pages, the key community interaction forum where the contributing community interacts or discusses or debates about the changes pertaining to a particular topic. \n",
        "  - Wikipedia continuously strives to help online discussion become more productive and respectful. You are a data scientist at Wikipedia who will help Wikipedia to build a predictive model that identifies toxic comments in the discussion and marks them for cleanup by using NLP and machine learning. Post that, help identify the top terms from the toxic comments. \n",
        "\n",
        "#### **Analysis to be done:**\n",
        "- Build a text classification model using NLP and machine learning that detects toxic comments.\n",
        "\n",
        "#### **Data Dictionary:** \n",
        "- id: identifier number of the comment\n",
        "- comment_text: the text in the comment\n",
        "- toxic: 0 (non-toxic) / 1 (toxic)\n",
        "\n",
        "#### **Steps to perform:**\n",
        "- Cleanup the text data, using TF-IDF convert to vector space representation, use Support Vector Machines to detect toxic comments. \n",
        "- Finally, get the list of top 15 toxic terms from the comments identified by the model.\n",
        "\n",
        "#### **Sub-Tasks:** \n",
        "- Load the data using read_csv function from pandas package\n",
        "- Get the comments into a list, for easy text cleanup and manipulation\n",
        "- Cleanup: \n",
        "  - Using regular expressions, remove IP addresses\n",
        "  - Using regular expressions, remove URLs\n",
        "  - Normalize the casing\n",
        "  - Tokenize using word_tokenize from NLTK\n",
        "  - Remove stop words\n",
        "  - Remove punctuation\n",
        "  - Define a function to perform all these steps, you’ll use this later on the actual test set\n",
        "- Using a counter, find the top terms in the data.\n",
        "  - Can any of these be considered contextual stop words?\n",
        "  - Words like “Wikipedia”, “page”, “edit” are examples of contextual stop words\n",
        "  - If yes, drop these from the data\n",
        "- Separate into train and test sets\n",
        "  - Use train-test method to divide your data into 2 sets: train and test\n",
        "  - Use a 70-30 split\n",
        "- Use TF-IDF values for the terms as feature to get into a vector space model\n",
        "  - Import TF-IDF vectorizer from sklearn\n",
        "  - Instantiate with a maximum of 4000 terms in your vocabulary\n",
        "  - Fit and apply on the train set\n",
        "  - Apply on the test set\n",
        "- Model building: Support Vector Machine\n",
        "  - Instantiate SVC from sklearn with a linear kernel\n",
        "  - Fit on the train data\n",
        "  - Make predictions for the train and the test set\n",
        "- Model evaluation: Accuracy, recall, and f1_score\n",
        "  - Report the accuracy on the train set\n",
        "  - Report the recall on the train set:decent, high, low?\n",
        "  - Get the f1_score on the train set\n",
        "- Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s\n",
        "  - Adjust the appropriate parameter in the SVC module\n",
        "- Train again with the adjustment and evaluate\n",
        "  - Train the model on the train set\n",
        "  - Evaluate the predictions on the validation set: accuracy, recall, f1_score\n",
        "- Hyperparameter tuning\n",
        "  - Import GridSearch and StratifiedKFold (because of class imbalance)\n",
        "  - Provide the parameter grid to choose for ‘C’\n",
        "  - Use a balanced class weight while instantiating the Support Vector Classifier\n",
        "- Find the parameters with the best recall in cross validation\n",
        "  - Choose ‘recall’ as the metric for scoring\n",
        "  - Choose stratified 5 fold cross validation scheme\n",
        "  - Fit on the train set\n",
        "- What are the best parameters?\n",
        "- Predict and evaluate using the best estimator\n",
        "  - Use best estimator from the grid search to make predictions on the test set\n",
        "  - What is the recall on the test set for the toxic comments?\n",
        "  - What is the f1_score?\n",
        "- What are the most prominent terms in the toxic comments?\n",
        "  - Separate the comments from the test set that the model identified as toxic\n",
        "  - Make one large list of the terms\n",
        "  - Get the top 15 terms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aquire the data**"
      ],
      "metadata": {
        "id": "lf9_-E4lqsry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKcJPgsKqgT0"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/snikhil17/NLP_Simplilearn/main/capstone_project_2_wikipedia_toxicity/train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading required Libraries**"
      ],
      "metadata": {
        "id": "rKCEks2Mq9ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "uBrJDxoTq_Qm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcUayBD8rBO7",
        "outputId": "4e01326f-3be5-4b26-b504-6bff4a68ab32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4Qv_G0yhrCO6",
        "outputId": "3622d1e1-b49c-4bcf-97b3-8a0b3f2ef04e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-01449481-149c-4997-83cb-4e335e497559\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e617e2489abe9bca</td>\n",
              "      <td>\"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9250cf637294e09d</td>\n",
              "      <td>\"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ce1aa4592d5240ca</td>\n",
              "      <td>Marya Dzmitruk was born in Minsk, Belarus in M...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48105766ff7f075b</td>\n",
              "      <td>\"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0543d4f82e5470b6</td>\n",
              "      <td>New Categories \\r\\n\\r\\nI honestly think that w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01449481-149c-4997-83cb-4e335e497559')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01449481-149c-4997-83cb-4e335e497559 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01449481-149c-4997-83cb-4e335e497559');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic\n",
              "0  e617e2489abe9bca  \"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...      0\n",
              "1  9250cf637294e09d  \"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...      0\n",
              "2  ce1aa4592d5240ca  Marya Dzmitruk was born in Minsk, Belarus in M...      0\n",
              "3  48105766ff7f075b      \"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"      0\n",
              "4  0543d4f82e5470b6  New Categories \\r\\n\\r\\nI honestly think that w...      0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning Helper Functions** \n",
        "- **Function for decontracted and removing IP adresses and other expressions using Regex**\n",
        "  - words like don't , won't etc will be converted to do not, will not etc.\n",
        "  - Emojis, additional lines, email-addresses, website names etc are removed here.\n",
        "- **Removed words like ``no, not, nor`` from english stopwords**\n",
        "- **Removing Non-English Words**\n",
        "  - whatever words nltk corpus has, if given word is in that corpus we will consider the word, else replace with blank.\n",
        "- **Removing Contextual Stopwords**\n",
        "  - Using POS-tagging feature of Spacy, removed words which were Pronoun, punctuation, number, adverb  etc.\n",
        "  - Lemmatizing th words and checking if the given word exist in the list of words needs to be removed (contextual words: obtained after checking the word counts)\n",
        "- **Remaining Preprocessing of text**\n",
        "  - Combining all the above functions to use as analyzer in TF-IDF vectorizer. "
      ],
      "metadata": {
        "id": "jZwLmH9grFv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Functions to clean text using Regex\"\"\"\n",
        "ip_addr_regex = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "def regex_cleaning(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"[^a-zA-Z0-9]+\", \" \", phrase)\n",
        "    phrase = re.sub(r\"\\r\\n\", \"\", phrase)            # Removing additional line\n",
        "    phrase = re.sub(r\"\\n\", \"\", phrase)              # Removing additional line \n",
        "    phrase = re.sub(r\"\\S*@\\S*\\s?\", \"\", phrase)      # Removing email-addresses \n",
        "    phrase = re.sub(r'http\\S+', '', phrase)         # Removing website links\n",
        "    phrase = re.sub(ip_addr_regex, \"\", phrase)      # Removing IP address link.\n",
        "    phrase = emoji_pattern.sub(r'', phrase)         # Removing Emojis\n",
        "    \n",
        "    return phrase.lower() \n",
        "\n",
        "\"\"\"Ammending Stopwords list\"\"\"\n",
        "stop_words = stopwords.words('english')\n",
        "for i in ['nor', 'not', 'no']:\n",
        "  stop_words.remove(i)\n",
        "\n",
        "\"\"\"Removing Non-English Words\"\"\"\n",
        "nltk_words = set(nltk.corpus.words.words())\n",
        "def only_eng(row):\n",
        "  for word in row:\n",
        "    if word in nltk_words or not word.isalpha():\n",
        "      word = word\n",
        "    else:\n",
        "      word = \"\"\n",
        "  return row\n",
        "\n",
        "\"\"\"Helper function for lemmatization_check\"\"\"\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# \"\"\"Extracting Lemmatized word to take care that words like page, paged, pages can be removed at once.\"\"\"\n",
        "\n",
        "def lemmatization_check(word):\n",
        "  word_pos = nltk.pos_tag(word)\n",
        "  lemma_word = lemmaObject.lemmatize(word,get_wordnet_pos(word))\n",
        "  return lemma_word\n",
        "\n",
        "\"\"\"Removing Contextual Stopwords\"\"\"\n",
        "# 'ADJ',\n",
        "lemmaObject = WordNetLemmatizer()\n",
        "pos_taggings_to_remove = ['DET', 'ADP','ADV','AUX', 'SCONJ',  'INTJ','PUNCT', 'NUM',  'SPACE', 'X']\n",
        "some_words_to_remove = ['you', 'wikipedia', 'wiki' ,'edit', 'page', 'would', 'article', 'articles','page', 'edits', 'user']\n",
        "def remove_contextual_stopwords(row):\n",
        "  row= row.lower()  \n",
        "  final_row = []\n",
        "  row = \"\".join([lemmaObject.lemmatize(word[0],get_wordnet_pos(word[1])) for word in nltk.pos_tag(row)])\n",
        "  row = nlp(row)\n",
        "  for word in row:    \n",
        "    if word.pos_ not in pos_taggings_to_remove and word.text not in some_words_to_remove:\n",
        "      final_row.append(word.text)\n",
        "    else:\n",
        "      continue\n",
        "  return \" \".join(final_row)\n",
        "\n",
        "\"\"\"Remaining Preprocessing of text\"\"\"\n",
        "def final_preprocessing(document):\n",
        "  document_regex_cleaned = regex_cleaning(document)\n",
        "  noNonEnglishWords = only_eng(document_regex_cleaned)\n",
        "  words = [\" \".join(nltk.word_tokenize(title)) for title in noNonEnglishWords.split()]                                         # Word Tokenization0\n",
        "  wordWithoutStopwords = [word for word in words if word not in stop_words if len(word) > 3]                                   # Remove Stopwords\n",
        "  vocabulary = \" \".join([char for char in wordWithoutStopwords if char not in string.punctuation ])                            # Remove Punctuations\n",
        "  noContextualStopwords = remove_contextual_stopwords(vocabulary)                                                              # Remove Contextual Stopwords\n",
        "  return noContextualStopwords.split()                                                                                         # Returning lemmatized-Vocabulary"
      ],
      "metadata": {
        "id": "i7QQQ7T7rEIn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L1mamHLJrPCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "wikiToxicity.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}