{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snikhil17/NLP_Simplilearn/blob/main/capstone_project_2_wikipedia_toxicity/wikiToxicity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG0asDoiqgTy"
      },
      "source": [
        "## **Problem Context**\n",
        "#### **Description:** \n",
        "- Using NLP and machine learning, make a model to identify toxic comments from the Talk edit pages on Wikipedia. Help identify the words that make a comment toxic.\n",
        "\n",
        "#### **Statement:**  \n",
        "- Wikipedia is the world’s largest and most popular reference work on the internet with about 500 million unique visitors per month. It also has millions of contributors who can make edits to pages. The Talk edit pages, the key community interaction forum where the contributing community interacts or discusses or debates about the changes pertaining to a particular topic. \n",
        "  - Wikipedia continuously strives to help online discussion become more productive and respectful. You are a data scientist at Wikipedia who will help Wikipedia to build a predictive model that identifies toxic comments in the discussion and marks them for cleanup by using NLP and machine learning. Post that, help identify the top terms from the toxic comments. \n",
        "\n",
        "#### **Analysis to be done:**\n",
        "- Build a text classification model using NLP and machine learning that detects toxic comments.\n",
        "\n",
        "#### **Data Dictionary:** \n",
        "- id: identifier number of the comment\n",
        "- comment_text: the text in the comment\n",
        "- toxic: 0 (non-toxic) / 1 (toxic)\n",
        "\n",
        "#### **Steps to perform:**\n",
        "- Cleanup the text data, using TF-IDF convert to vector space representation, use Support Vector Machines to detect toxic comments. \n",
        "- Finally, get the list of top 15 toxic terms from the comments identified by the model.\n",
        "\n",
        "#### **Sub-Tasks:** \n",
        "- Load the data using read_csv function from pandas package\n",
        "- Get the comments into a list, for easy text cleanup and manipulation\n",
        "- Cleanup: \n",
        "  - Using regular expressions, remove IP addresses\n",
        "  - Using regular expressions, remove URLs\n",
        "  - Normalize the casing\n",
        "  - Tokenize using word_tokenize from NLTK\n",
        "  - Remove stop words\n",
        "  - Remove punctuation\n",
        "  - Define a function to perform all these steps, you’ll use this later on the actual test set\n",
        "- Using a counter, find the top terms in the data.\n",
        "  - Can any of these be considered contextual stop words?\n",
        "  - Words like “Wikipedia”, “page”, “edit” are examples of contextual stop words\n",
        "  - If yes, drop these from the data\n",
        "- Separate into train and test sets\n",
        "  - Use train-test method to divide your data into 2 sets: train and test\n",
        "  - Use a 70-30 split\n",
        "- Use TF-IDF values for the terms as feature to get into a vector space model\n",
        "  - Import TF-IDF vectorizer from sklearn\n",
        "  - Instantiate with a maximum of 4000 terms in your vocabulary\n",
        "  - Fit and apply on the train set\n",
        "  - Apply on the test set\n",
        "- Model building: Support Vector Machine\n",
        "  - Instantiate SVC from sklearn with a linear kernel\n",
        "  - Fit on the train data\n",
        "  - Make predictions for the train and the test set\n",
        "- Model evaluation: Accuracy, recall, and f1_score\n",
        "  - Report the accuracy on the train set\n",
        "  - Report the recall on the train set:decent, high, low?\n",
        "  - Get the f1_score on the train set\n",
        "- Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s\n",
        "  - Adjust the appropriate parameter in the SVC module\n",
        "- Train again with the adjustment and evaluate\n",
        "  - Train the model on the train set\n",
        "  - Evaluate the predictions on the validation set: accuracy, recall, f1_score\n",
        "- Hyperparameter tuning\n",
        "  - Import GridSearch and StratifiedKFold (because of class imbalance)\n",
        "  - Provide the parameter grid to choose for ‘C’\n",
        "  - Use a balanced class weight while instantiating the Support Vector Classifier\n",
        "- Find the parameters with the best recall in cross validation\n",
        "  - Choose ‘recall’ as the metric for scoring\n",
        "  - Choose stratified 5 fold cross validation scheme\n",
        "  - Fit on the train set\n",
        "- What are the best parameters?\n",
        "- Predict and evaluate using the best estimator\n",
        "  - Use best estimator from the grid search to make predictions on the test set\n",
        "  - What is the recall on the test set for the toxic comments?\n",
        "  - What is the f1_score?\n",
        "- What are the most prominent terms in the toxic comments?\n",
        "  - Separate the comments from the test set that the model identified as toxic\n",
        "  - Make one large list of the terms\n",
        "  - Get the top 15 terms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Aquire the data**"
      ],
      "metadata": {
        "id": "lf9_-E4lqsry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QKcJPgsKqgT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0506d0cb-71c1-4a5c-e1f1-eb8b40976144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-26 07:40:05--  https://raw.githubusercontent.com/snikhil17/NLP_Simplilearn/main/capstone_project_2_wikipedia_toxicity/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2102032 (2.0M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   2.00M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-12-26 07:40:06 (25.4 MB/s) - ‘train.csv’ saved [2102032/2102032]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/snikhil17/NLP_Simplilearn/main/capstone_project_2_wikipedia_toxicity/train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading required Libraries**"
      ],
      "metadata": {
        "id": "rKCEks2Mq9ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "uBrJDxoTq_Qm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcUayBD8rBO7",
        "outputId": "96e5b687-4944-447d-c02e-5b68a84ee758"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4Qv_G0yhrCO6",
        "outputId": "603067ff-de62-42b9-952c-e2d5dcf1b870"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6faa7534-be6c-449b-aa76-fbb090f19490\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e617e2489abe9bca</td>\n",
              "      <td>\"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9250cf637294e09d</td>\n",
              "      <td>\"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ce1aa4592d5240ca</td>\n",
              "      <td>Marya Dzmitruk was born in Minsk, Belarus in M...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48105766ff7f075b</td>\n",
              "      <td>\"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0543d4f82e5470b6</td>\n",
              "      <td>New Categories \\r\\n\\r\\nI honestly think that w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6faa7534-be6c-449b-aa76-fbb090f19490')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6faa7534-be6c-449b-aa76-fbb090f19490 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6faa7534-be6c-449b-aa76-fbb090f19490');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic\n",
              "0  e617e2489abe9bca  \"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...      0\n",
              "1  9250cf637294e09d  \"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...      0\n",
              "2  ce1aa4592d5240ca  Marya Dzmitruk was born in Minsk, Belarus in M...      0\n",
              "3  48105766ff7f075b      \"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"      0\n",
              "4  0543d4f82e5470b6  New Categories \\r\\n\\r\\nI honestly think that w...      0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning Helper Functions** \n",
        "- **Function for decontracted and removing IP adresses and other expressions using Regex**\n",
        "  - words like don't , won't etc will be converted to do not, will not etc.\n",
        "  - Emojis, additional lines, email-addresses, website names etc are removed here.\n",
        "- **Removed words like ``no, not, nor`` from english stopwords**\n",
        "- **Removing Non-English Words**\n",
        "  - whatever words nltk corpus has, if given word is in that corpus we will consider the word, else replace with blank.\n",
        "- **Removing Contextual Stopwords**\n",
        "  - Using POS-tagging feature of Spacy, removed words which were Pronoun, punctuation, number, adverb  etc.\n",
        "  - Lemmatizing th words and checking if the given word exist in the list of words needs to be removed (contextual words: obtained after checking the word counts)\n",
        "- **Remaining Preprocessing of text**\n",
        "  - Combining all the above functions to use as analyzer in TF-IDF vectorizer. "
      ],
      "metadata": {
        "id": "jZwLmH9grFv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Functions to clean text using Regex\"\"\"\n",
        "ip_addr_regex = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "def regex_cleaning(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"[^a-zA-Z0-9]+\", \" \", phrase)\n",
        "    phrase = re.sub(r\"\\r\\n\", \"\", phrase)            # Removing additional line\n",
        "    phrase = re.sub(r\"\\n\", \"\", phrase)              # Removing additional line \n",
        "    phrase = re.sub(r\"\\S*@\\S*\\s?\", \"\", phrase)      # Removing email-addresses \n",
        "    phrase = re.sub(r'http\\S+', '', phrase)         # Removing website links\n",
        "    phrase = re.sub(ip_addr_regex, \"\", phrase)      # Removing IP address link.\n",
        "    phrase = emoji_pattern.sub(r'', phrase)         # Removing Emojis\n",
        "    \n",
        "    return phrase.lower() \n",
        "\n",
        "\"\"\"Ammending Stopwords list\"\"\"\n",
        "stop_words = stopwords.words('english')\n",
        "for i in ['nor', 'not', 'no']:\n",
        "  stop_words.remove(i)\n",
        "\n",
        "\"\"\"Removing Non-English Words\"\"\"\n",
        "nltk_words = set(nltk.corpus.words.words())\n",
        "def only_eng(row):\n",
        "  for word in row:\n",
        "    if word in nltk_words or not word.isalpha():\n",
        "      word = word\n",
        "    else:\n",
        "      word = \"\"\n",
        "  return row\n",
        "\n",
        "\"\"\"Helper function for lemmatization_check\"\"\"\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# \"\"\"Extracting Lemmatized word to take care that words like page, paged, pages can be removed at once.\"\"\"\n",
        "\n",
        "def lemmatization_check(word):\n",
        "  word_pos = nltk.pos_tag(word)\n",
        "  lemma_word = lemmaObject.lemmatize(word,get_wordnet_pos(word))\n",
        "  return lemma_word\n",
        "\n",
        "\"\"\"Removing Contextual Stopwords\"\"\"\n",
        "# 'ADJ',\n",
        "lemmaObject = WordNetLemmatizer()\n",
        "pos_taggings_to_remove = ['DET', 'ADP','ADV','AUX', 'SCONJ',  'INTJ','PUNCT', 'NUM',  'SPACE', 'X']\n",
        "some_words_to_remove = ['you', 'wikipedia', 'wiki' ,'edit', 'page', 'would', 'article', 'articles','page', 'edits', 'user']\n",
        "def remove_contextual_stopwords(row):\n",
        "  row= row.lower()  \n",
        "  final_row = []\n",
        "  row = \"\".join([lemmaObject.lemmatize(word[0],get_wordnet_pos(word[1])) for word in nltk.pos_tag(row)])\n",
        "  row = nlp(row)\n",
        "  for word in row:    \n",
        "    if word.pos_ not in pos_taggings_to_remove and word.text not in some_words_to_remove:\n",
        "      final_row.append(word.text)\n",
        "    else:\n",
        "      continue\n",
        "  return \" \".join(final_row)\n",
        "\n",
        "\"\"\"Remaining Preprocessing of text\"\"\"\n",
        "def final_preprocessing(document):\n",
        "  document_regex_cleaned = regex_cleaning(document)\n",
        "  noNonEnglishWords = only_eng(document_regex_cleaned)\n",
        "  words = [\" \".join(nltk.word_tokenize(title)) for title in noNonEnglishWords.split()]                                         # Word Tokenization0\n",
        "  wordWithoutStopwords = [word for word in words if word not in stop_words if len(word) > 3]                                   # Remove Stopwords\n",
        "  vocabulary = \" \".join([char for char in wordWithoutStopwords if char not in string.punctuation ])                            # Remove Punctuations\n",
        "  noContextualStopwords = remove_contextual_stopwords(vocabulary)                                                              # Remove Contextual Stopwords\n",
        "  return noContextualStopwords.split()                                                                                         # Returning lemmatized-Vocabulary"
      ],
      "metadata": {
        "id": "i7QQQ7T7rEIn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Helper functions to clean the data**"
      ],
      "metadata": {
        "id": "2cs9iBa_rq6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text'] = df['comment_text'].apply(regex_cleaning)  # Cleaning using Regex\n",
        "df['cleaned_text'] = df['comment_text'].apply(only_eng)  # Selecting Only English words"
      ],
      "metadata": {
        "id": "L1mamHLJrPCI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contextual Stop words**\n",
        "- Creating a for loop to store all the comments into a single list:\n",
        "- In terms of Preprocessing only steps used are remove words less than 3 characters and removed stopwords\n",
        "- The result is stored in a dictionary, where key is word and value is frequency of the word. \n",
        "- This dictionary is convered into DataFrame to observe most frequent words and using that decision is made about selecting **Contextual Stop Words**"
      ],
      "metadata": {
        "id": "XM_G6CAgrxiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict ={}\n",
        "text_list = [text for text in df['comment_text']]\n",
        "for sentence in text_list:\n",
        "  for word in sentence.split(): \n",
        "    \n",
        "    if word in freq_dict.keys() and len(word) > 3 and word not in stopwords.words('english'):\n",
        "      freq_dict[word] += 1\n",
        "\n",
        "    if word not in freq_dict.keys() and len(word) > 3 and word not in stopwords.words('english'):\n",
        "      freq_dict[word] = 1\n",
        "\n",
        "\n",
        "count_df = pd.DataFrame( freq_dict.values(), freq_dict.keys()).reset_index().rename({0: \"Frequency\", \"index\": \"Words\"}, axis =1)\n",
        "count_df.sort_values(by = 'Frequency', ascending = False)[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "HMuGBRvUrwTK",
        "outputId": "c57faf13-a7d2-4ce6-ac66-e12f25526d5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fbcbba92-627a-463f-9efa-4d66f892e9a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>article</td>\n",
              "      <td>1137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>would</td>\n",
              "      <td>931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>page</td>\n",
              "      <td>924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>like</td>\n",
              "      <td>764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>ass.</td>\n",
              "      <td>682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>talk</td>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>fuck</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>think</td>\n",
              "      <td>589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>Please</td>\n",
              "      <td>575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>Wikipedia</td>\n",
              "      <td>544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>know</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>This</td>\n",
              "      <td>464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>also</td>\n",
              "      <td>453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>edit</td>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>people</td>\n",
              "      <td>422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbcbba92-627a-463f-9efa-4d66f892e9a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbcbba92-627a-463f-9efa-4d66f892e9a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbcbba92-627a-463f-9efa-4d66f892e9a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Words  Frequency\n",
              "14     article       1137\n",
              "54       would        931\n",
              "446       page        924\n",
              "5         like        764\n",
              "718       ass.        682\n",
              "584       talk        652\n",
              "684       fuck        592\n",
              "94       think        589\n",
              "570     Please        575\n",
              "589  Wikipedia        544\n",
              "800       know        500\n",
              "29        This        464\n",
              "188       also        453\n",
              "6         edit        451\n",
              "85      people        422"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observations:**\n",
        "---\n",
        "- We can clearly see that words like **page, wikipedia, edit** show up in top 15 words (in terms of frequency).\n",
        "- We can easily call these words as **Contextual Stop-words** and remove such words from our corpus.\n",
        "- Using Part of Speech, certain POS (Adjective, Pronoun, Number, Pronoun, Punctuation) words are removed as they are also be considered as **Contextual Stop Words** \n",
        "\n",
        "### **Using remove_contextual_stopwords function:**\n",
        "---\n",
        "- Remove Contextual words from corpus.\n"
      ],
      "metadata": {
        "id": "PKOS7zUxr0hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df['cleaned_text'] = df['comment_text'].apply(remove_contextual_stopwords) # Removing Contextual Stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU-n9rW1ry9F",
        "outputId": "dbe01589-9c2c-4097-83f3-82b69d29c4c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 8s, sys: 2.33 s, total: 4min 10s\n",
            "Wall time: 4min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's check the most-frequent words now:**\n",
        "- Same method as used above"
      ],
      "metadata": {
        "id": "MH-RA6TEr3uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict ={}\n",
        "text_list = [text for text in df['cleaned_text']]\n",
        "for sentence in text_list:\n",
        "  for word in sentence.split(): \n",
        "    \n",
        "    if word in freq_dict.keys() and len(word) > 3 and word not in stopwords.words('english'):\n",
        "      freq_dict[word] += 1\n",
        "\n",
        "    if word not in freq_dict.keys() and len(word) > 3 and word not in stopwords.words('english'):\n",
        "      freq_dict[word] = 1\n",
        "\n",
        "\n",
        "count_df = pd.DataFrame( freq_dict.values(), freq_dict.keys()).reset_index().rename({0: \"Frequency\", \"index\": \"Words\"}, axis =1)\n",
        "count_df.sort_values(by = 'Frequency', ascending = False)[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "XskISsfNr2ey",
        "outputId": "eb01a520-fc67-4845-ab31-8135b6e71475"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4563ac90-8660-4b2b-a01d-b8b02ef7ca84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>talk</td>\n",
              "      <td>1182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>think</td>\n",
              "      <td>630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>fuck</td>\n",
              "      <td>629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>know</td>\n",
              "      <td>596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>people</td>\n",
              "      <td>553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>name</td>\n",
              "      <td>540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>time</td>\n",
              "      <td>486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>thanks</td>\n",
              "      <td>416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>could</td>\n",
              "      <td>394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>information</td>\n",
              "      <td>393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1024</th>\n",
              "      <td>deletion</td>\n",
              "      <td>391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>make</td>\n",
              "      <td>391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>good</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2655</th>\n",
              "      <td>suck</td>\n",
              "      <td>375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>961</th>\n",
              "      <td>image</td>\n",
              "      <td>367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4563ac90-8660-4b2b-a01d-b8b02ef7ca84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4563ac90-8660-4b2b-a01d-b8b02ef7ca84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4563ac90-8660-4b2b-a01d-b8b02ef7ca84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            Words  Frequency\n",
              "225          talk       1182\n",
              "67          think        630\n",
              "418          fuck        629\n",
              "45           know        596\n",
              "60         people        553\n",
              "13           name        540\n",
              "62           time        486\n",
              "453        thanks        416\n",
              "167         could        394\n",
              "368   information        393\n",
              "1024     deletion        391\n",
              "189          make        391\n",
              "57           good        377\n",
              "2655         suck        375\n",
              "961         image        367"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observation:**\n",
        "- Above result actually makes sense. as most of our data is non-toxic comment, our data now shows that a few toxic words are present in the top 15 most frequenct words.\n"
      ],
      "metadata": {
        "id": "rY5Obd2otMM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df['comment_text'].values\n",
        "labels = df['toxic'].values"
      ],
      "metadata": {
        "id": "F2Kfd_fgtKkd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's Try BoW on subset of Data**"
      ],
      "metadata": {
        "id": "TRkMtfH8tPTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BOW in SKlearn\n",
        "wordVector = CountVectorizer(analyzer = final_preprocessing)\n",
        "\n",
        "#Build the Vocabulary\n",
        "finalWordVectorVocab = wordVector.fit(features[:10])\n",
        "\n",
        "# # To create BOW\n",
        "bagOfWords = finalWordVectorVocab.transform(features[:10])"
      ],
      "metadata": {
        "id": "iNMq8o6TtN8E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finalWordVectorVocab.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2-PeyLktQgn",
        "outputId": "7345141b-e4f4-4cb1-d695-f1413be1b566"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abuse' 'account' 'admin' 'administrative' 'admitted' 'alex' 'american'\n",
            " 'anyone' 'astrological' 'attends' 'attention' 'august' 'awards'\n",
            " 'baranovichi' 'barnstar' 'began' 'behavior' 'belarus' 'belarusfilm'\n",
            " 'best' 'block' 'born' 'brest' 'bring' 'call' 'called' 'can' 'castist'\n",
            " 'categories' 'cause' 'celestia' 'certain' 'characters' 'child' 'children'\n",
            " 'class' 'classes' 'close' 'committee' 'competition' 'continue'\n",
            " 'contribute' 'contributing' 'could' 'course' 'creating' 'dating' 'deals'\n",
            " 'dear' 'deceased' 'decided' 'defender' 'denied' 'deserve' 'diplomas'\n",
            " 'disagree' 'discussion' 'dislike' 'divorced' 'drama' 'dumbhead'\n",
            " 'dunechka' 'dzmitruk' 'editor' 'editors' 'ethinicty' 'expect' 'extreme'\n",
            " 'family' 'father' 'featured' 'filtering' 'first' 'folloing' 'followed'\n",
            " 'fordman' 'form' 'former' 'freak' 'free' 'frequency' 'front' 'good'\n",
            " 'government' 'graduate' 'graduated' 'greater' 'group' 'guess' 'gwern'\n",
            " 'gymnastics' 'hands' 'happy' 'harass' 'held' 'helsinki' 'high' 'higher'\n",
            " 'hippie' 'hits' 'holds' 'human' 'inappropriate' 'ingrid' 'institute'\n",
            " 'intellectuals' 'international' 'isar' 'issue' 'kayastha' 'know' 'known'\n",
            " 'lacy' 'leadership' 'leave' 'lets' 'life' 'like' 'lisa' 'listed' 'looks'\n",
            " 'loser' 'ludwigs2' 'main' 'make' 'malign' 'many' 'march' 'marked' 'marya'\n",
            " 'mathsci' 'matter' 'medal' 'member' 'mention' 'middle' 'minsk' 'months'\n",
            " 'moroz' 'mother' 'move' 'movie' 'music' 'must' 'name' 'need' 'nikolaevna'\n",
            " 'not' 'oceana' 'odessa' 'olga' 'opinion' 'organization' 'others'\n",
            " 'parents' 'people' 'personal' 'pete' 'piano' 'plan' 'play' 'point' 'post'\n",
            " 'problem' 'profit' 'project' 'prone' 'propose' 'public' 'punished' 'quit'\n",
            " 'racist' 'reason' 'reign' 'relationship' 'remarried' 'request' 'research'\n",
            " 'riding' 'rights' 'ross' 'rude' 'ruth' 'said' 'says' 'scholarships'\n",
            " 'school' 'schools' 'second' 'section' 'seems' 'seen' 'sense' 'september'\n",
            " 'several' 'show' 'shows' 'sites' 'situations' 'society' 'solidarity'\n",
            " 'source' 'spaces' 'spector' 'spring' 'standards' 'starred' 'start'\n",
            " 'started' 'states' 'stop' 'subject' 'talk' 'talkback' 'television'\n",
            " 'theatre' 'things' 'think' 'time' 'told' 'traveled' 'trying' 'turned'\n",
            " 'uifkearca' 'ukraine' 'unbalanced' 'united' 'unpleasant' 'used' 'violin'\n",
            " 'warned' 'went' 'whitney' 'willing' 'works' 'world' 'worse' 'years'\n",
            " 'youth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train-Test Split and instantiating TF-IDF Vectorizer:**\n",
        "- Used **stratify = labels** in train_test_split so that imbalanced ratio is mentained in train-test-split.\n",
        "- Used **max_features = 4000** to restrict the vocabulary to 4000.\n",
        "- Used **min_df = 2** which selects only those words which atleast are seen 2 times.\n",
        "- Fit and Transformed X_train and Transformed X_test"
      ],
      "metadata": {
        "id": "ErUdjec_tTD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Splitting the data into Training and Testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, stratify = labels,   test_size= 0.3,  random_state = 15)\n",
        "\n",
        "#Create TF-IDF using TfidfVectorizer\n",
        "tfidfVectorizer = TfidfVectorizer(analyzer= final_preprocessing, max_features = 4000, min_df = 2) # Instantiating Tfidfvectorizer\n",
        "tfidfObject_train = tfidfVectorizer.fit_transform(X_train)                                        # Fit & transform on Training set\n",
        "tfidfObject_test = tfidfVectorizer.transform(X_test)                                              # Transform the testing set\n",
        "print(tfidfObject_train.shape, tfidfObject_test.shape)                                            # Checking Shape of training and testing set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q0Y1X6wtRl-",
        "outputId": "ae7af51f-402b-490b-a0ad-b9ddba3d858f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3500, 4000) (1500, 4000)\n",
            "CPU times: user 3min 31s, sys: 988 ms, total: 3min 32s\n",
            "Wall time: 3min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create first model: SVC**\n",
        "- SVC model with kernel = \"linear\" which is used for binary classification.\n",
        "- Class imbalance of label is not considered here."
      ],
      "metadata": {
        "id": "3pTzfRFQtWS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(tfidfObject_train,y_train)\n",
        "\n",
        "preds_train = model.predict(tfidfObject_train)                                            # Prediction on training set\n",
        "preds_test = model.predict(tfidfObject_test)                                              # Prediction on training set\n",
        "\n",
        "training_metrics = metrics.classification_report(y_train, preds_train)                    # Getting Classification report of Training set predictions\n",
        "testing_metrics = metrics.classification_report(y_test, preds_test)                       # Getting Classification report of Test set predictions\n",
        "\n",
        "print(f\"Training Confusion Matrix: \\n{metrics.confusion_matrix(y_train, preds_train)}\\n\") # Printing Confusion Matrix (training)\n",
        "print(f\"Training Classification Report: \\n{training_metrics}\")                            # Printing Classification report (testing)\n",
        "print(\"=\"*100)\n",
        "print(f\"Testing Confusion Matrix: \\n{metrics.confusion_matrix(y_test, preds_test)}\\n\")    # Printing Confusion Matrix (training)\n",
        "print(f\"Testing Classification Report: \\n{testing_metrics}\")                              # Printing Classification report (testing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znqwQb2RtVBZ",
        "outputId": "5304c3a3-c1a0-453e-b7e5-6ebbeb0ae295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Confusion Matrix: \n",
            "[[3193    1]\n",
            " [ 128  178]]\n",
            "\n",
            "Training Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      3194\n",
            "           1       0.99      0.58      0.73       306\n",
            "\n",
            "    accuracy                           0.96      3500\n",
            "   macro avg       0.98      0.79      0.86      3500\n",
            "weighted avg       0.96      0.96      0.96      3500\n",
            "\n",
            "====================================================================================================\n",
            "Testing Confusion Matrix: \n",
            "[[1366    3]\n",
            " [  74   57]]\n",
            "\n",
            "Testing Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97      1369\n",
            "           1       0.95      0.44      0.60       131\n",
            "\n",
            "    accuracy                           0.95      1500\n",
            "   macro avg       0.95      0.72      0.78      1500\n",
            "weighted avg       0.95      0.95      0.94      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observations:**\n",
        "- Model is not generalized, training_accuracy > testing_accuracy\n",
        "- 0 = Non-Toxic, 1 = Toxic\n",
        "- Observing Confusion matrix and Recall we can see that Model is classifying Non-toxic comments better than toxic comments, reason could be unbalanced data.\n",
        "- Moreover model is predicting only 1 non-toxic comment as toxic comment, whereas it is predicting 128 toxic comments as non-toxic comments, which is not acceptable. Hence, imbalanced data needs to be taken care of.\n",
        "\n",
        "## **Model with taking care of Imbalanced Data:**\n",
        "- Using SVC again but this time using class-weight == 'balanced' which automatically takes care of the imbalanced class.\n",
        "- kernal == 'linear' as we know is used for binary classification."
      ],
      "metadata": {
        "id": "MWRR1o_VvO1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ykJG-2P3tmad"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "wikiToxicity.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}